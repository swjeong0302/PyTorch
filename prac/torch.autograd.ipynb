{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6d012e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet18, ResNet18_Weights # torchvision에서 학습된 ResNet18 모델을 불러옴. \n",
    "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "data = torch.rand(1, 3, 64, 64)\n",
    "labels = torch.rand(1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "76d84cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력(input) 데이터를 모델의 각 층(layer)에 통과시켜 예측값(prediction)을 생성\n",
    "prediction = model(data) # 순전파 단계(forward pass)를 수행하여 예측값(prediction)을 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b4dbaddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델의 예측값과 그에 해당하는 정답(label)을 사용하여 오차(error, 손실(loss))를 계산\n",
    "# 오차 텐서(error tensor)에 .backward() 메서드를 호출하여 모델의 매개변수(parameter)에 대한 손실(loss)의 그래디언트(gradient)를 계산\n",
    "loss = (prediction - labels).sum()\n",
    "loss.backward() # 역전파 단계(backward pass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "84b8422d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 옵티마이저(optimizer)에 모델의 모든 매개변수를 등록. 이 예제에서는 학습률(learning rate) 0.1과 모멘텀(momentum) 0.9를 갖는 SGD입니다. \n",
    "optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aab65153",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.step() # 옵티마이저(optimizer)의 step() 메서드를 호출하여 모델의 매개변수를 업데이트합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3044e09c",
   "metadata": {},
   "source": [
    "# Autograd에서 미분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9377d7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bc55320d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이제 a와 b로부터 새로운 텐서 Q를 만듦\n",
    "Q = 3*a**3 - b**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5416e839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([36., 81.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "external_grad = torch.tensor([1.,1.])\n",
    "Q.backward(gradient=external_grad) # 변화도는 a.grad와 b.grad에 저장됩니다.\n",
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "47624071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True])\n",
      "tensor([True, True])\n"
     ]
    }
   ],
   "source": [
    "# 수집된 변화도가 올바른지 확인합니다.\n",
    "print(9*a**2 == a.grad)\n",
    "print(-2*b == b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5645a7",
   "metadata": {},
   "source": [
    "# DAG에서 제외하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0a6d79eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does `a` require gradients?: False\n",
      "Does `b` require gradients?: True\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 5)\n",
    "y = torch.rand(5, 5)\n",
    "z = torch.rand((5, 5), requires_grad=True)\n",
    "\n",
    "a = x + y\n",
    "print(f\"Does `a` require gradients?: {a.requires_grad}\")\n",
    "b = x + z\n",
    "print(f\"Does `b` require gradients?: {b.requires_grad}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d70f1edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "model = resnet18(weights=ResNet18_Weights.DEFAULT) # 모델을 불러옴\n",
    "\n",
    "# 신경망의 모든 매개변수를 고정합니다\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False # 역전파를 통해 매개변수의 변화도가 계산되지 않도록 requires_grad를 False로 설정\n",
    "                                \n",
    "\n",
    "model.fc = nn.Linear(512, 10) # 새로운 분류기로 동작할 새로운 선형 계층으로 간단히 대체\n",
    "                              # 마지막 출력층을 관례적으로 fc라고 지어 둚. 그래서 모델의 마지막 계층을 새로 정의한 선형 계층으로 대체\n",
    "\n",
    "# 분류기만 최적화합니다.\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
